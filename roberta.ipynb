{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sassm1t/Detection-of-Clickbait-titles-using-deep-learning/blob/main/roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6Ihd4MQ0T4f",
        "outputId": "8a020a3b-2262-4b31-bdef-297fce78efeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Specify GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/content/merged_data.csv\")\n",
        "\n",
        "# Split dataset\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['headline'], df['clickbait'],\n",
        "                                                                  random_state=2018,\n",
        "                                                                  test_size=0.3,\n",
        "                                                                  stratify=df['clickbait'])\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2018,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)\n",
        "\n",
        "# Load RoBERTa tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Tokenize and encode sequences\n",
        "train_encodings = tokenizer(train_text.tolist(), padding=True, truncation=True, max_length=25)\n",
        "val_encodings = tokenizer(val_text.tolist(), padding=True, truncation=True, max_length=25)\n",
        "test_encodings = tokenizer(test_text.tolist(), padding=True, truncation=True, max_length=25)\n",
        "\n",
        "# Convert lists to tensors\n",
        "train_seq = torch.tensor(train_encodings['input_ids'])\n",
        "train_mask = torch.tensor(train_encodings['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(val_encodings['input_ids'])\n",
        "val_mask = torch.tensor(val_encodings['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(test_encodings['input_ids'])\n",
        "test_mask = torch.tensor(test_encodings['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(train_seq, train_mask, train_y)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_dataset = TensorDataset(val_seq, val_mask, val_y)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "# Load RoBERTa model\n",
        "roberta = AutoModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Freeze RoBERTa parameters\n",
        "for param in roberta.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define custom architecture on top of RoBERTa\n",
        "class RoBERTa_Arch(nn.Module):\n",
        "    def __init__(self, roberta):\n",
        "        super(RoBERTa_Arch, self).__init__()\n",
        "        self.roberta = roberta\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(roberta.config.hidden_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.fc1(pooled_output)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate model\n",
        "model = RoBERTa_Arch(roberta)\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training parameters\n",
        "epochs = 10  # Set a maximum number of epochs\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "best_val_loss = float('inf')\n",
        "best_model_state_dict = None\n",
        "patience = 3  # Number of epochs to wait before early stopping\n",
        "counter = 0  # Counter for early stopping\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f'Epoch {epoch + 1}:')\n",
        "    print(f'  Training Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'  Validation Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model_state_dict = model.state_dict()\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load best model state\n",
        "if best_model_state_dict is not None:\n",
        "    model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "\n",
        "# Define test data loader\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Evaluate on the test set\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wsZtGvB3GW6",
        "outputId": "ec2d649d-a664-40c1-95fc-8a26179eb9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "  Training Loss: 0.1154 | Accuracy: 0.9589\n",
            "  Validation Loss: 0.0613 | Accuracy: 0.9831\n",
            "Epoch 2:\n",
            "  Training Loss: 0.1102 | Accuracy: 0.9609\n",
            "  Validation Loss: 0.1209 | Accuracy: 0.9590\n",
            "Epoch 3:\n",
            "  Training Loss: 0.1151 | Accuracy: 0.9588\n",
            "  Validation Loss: 0.0726 | Accuracy: 0.9767\n",
            "Epoch 4:\n",
            "  Training Loss: 0.1088 | Accuracy: 0.9605\n",
            "  Validation Loss: 0.0637 | Accuracy: 0.9794\n",
            "Early stopping triggered.\n",
            "Test Accuracy: 0.9809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y9TBawo8LB_",
        "outputId": "c01a11ea-0704-43d8-d750-fdab6088e6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test data loader\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Evaluate on the test set\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Print classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=['Non-clickbait', 'Clickbait']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNYy3fH38pU7",
        "outputId": "5968013d-7700-42d1-fc33-2c66ab786b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9809\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Non-clickbait       0.99      0.98      0.98      3313\n",
            "    Clickbait       0.98      0.99      0.98      3245\n",
            "\n",
            "     accuracy                           0.98      6558\n",
            "    macro avg       0.98      0.98      0.98      6558\n",
            " weighted avg       0.98      0.98      0.98      6558\n",
            "\n"
          ]
        }
      ]
    }
  ]
}