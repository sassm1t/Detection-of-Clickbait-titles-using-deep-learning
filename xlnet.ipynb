{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sassm1t/Detection-of-Clickbait-titles-using-deep-learning/blob/main/xlnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Avw6TKP9tBZ",
        "outputId": "2b00158f-67ac-4452-b3bf-4d72ac9dae0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                headline  clickbait\n",
            "0                                     Should I Get Bings          1\n",
            "1          Which TV Female Friend Group Do You Belong In          1\n",
            "2      The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
            "3      This Vine Of New York On \"Celebrity Big Brothe...          1\n",
            "4      A Couple Did A Stunning Photo Shoot With Their...          1\n",
            "...                                                  ...        ...\n",
            "43712  To take on BJP, Trinamool likely to call meet ...          0\n",
            "43713  Kolkata: Four bodies found in field, no witnes...          0\n",
            "43714    Womans body found, Kolkata police await autopsy          0\n",
            "43715  Jewellery stores in Kolkata raided for accepti...          0\n",
            "43716         Kolkata: Fire at paper godown, no casualty          0\n",
            "\n",
            "[43717 rows x 2 columns]\n",
            "Epoch 1:\n",
            "  Training Loss: 0.0755 | Accuracy: 0.9715\n",
            "  Validation Loss: 0.0530 | Accuracy: 0.9863\n",
            "Epoch 2:\n",
            "  Training Loss: 0.0295 | Accuracy: 0.9902\n",
            "  Validation Loss: 0.0413 | Accuracy: 0.9886\n",
            "Epoch 3:\n",
            "  Training Loss: 0.0199 | Accuracy: 0.9939\n",
            "  Validation Loss: 0.0406 | Accuracy: 0.9866\n",
            "Epoch 4:\n",
            "  Training Loss: 0.0117 | Accuracy: 0.9967\n",
            "  Validation Loss: 0.0330 | Accuracy: 0.9928\n",
            "Test Accuracy: 0.9928\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Non-clickbait       0.99      0.99      0.99      3313\n",
            "    Clickbait       0.99      0.99      0.99      3245\n",
            "\n",
            "     accuracy                           0.99      6558\n",
            "    macro avg       0.99      0.99      0.99      6558\n",
            " weighted avg       0.99      0.99      0.99      6558\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import XLNetModel, XLNetTokenizerFast\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load dataset\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/content/merged_data.csv\"\n",
        "df = pd.read_csv(path)\n",
        "print(df)\n",
        "\n",
        "# Check class distribution\n",
        "df['clickbait'].value_counts(normalize=True)\n",
        "\n",
        "# Split train dataset\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(\n",
        "    df['headline'], df['clickbait'],\n",
        "    random_state=2018,\n",
        "    test_size=0.3,\n",
        "    stratify=df['clickbait'])\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(\n",
        "    temp_text, temp_labels,\n",
        "    random_state=2018,\n",
        "    test_size=0.5,\n",
        "    stratify=temp_labels)\n",
        "\n",
        "# Load the XLNet tokenizer\n",
        "tokenizer = XLNetTokenizerFast.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "# Tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length=25,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length=25,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length=25,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Convert lists to tensors\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())\n",
        "\n",
        "# Define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
        "\n",
        "# Samplers\n",
        "train_sampler = RandomSampler(train_data)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "\n",
        "# DataLoaders\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "# Load the XLNet model\n",
        "xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "class XLNet_Arch(nn.Module):\n",
        "    def __init__(self, xlnet):\n",
        "        super(XLNet_Arch, self).__init__()\n",
        "        self.xlnet = xlnet\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(768, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.xlnet(input_ids, attention_mask=attention_mask)\n",
        "        cls_hs = outputs.last_hidden_state[:, -1, :]\n",
        "        x = self.fc1(cls_hs)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = XLNet_Arch(xlnet)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "epochs = 4\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "patience = 2\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "\n",
        "    print(f'Epoch {epoch + 1}:')\n",
        "    print(f'  Training Loss: {avg_loss:.4f} | Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f'  Validation Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Checkpoint model if validation loss improved\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Load best model state\n",
        "if best_val_loss is not None:\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "# Evaluate on the test set\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Print classification report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=['Non-clickbait', 'Clickbait']))\n"
      ]
    }
  ]
}